{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22b67be2-549f-47fe-a250-b91e94a4a3a4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# Play with text-to-video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8c2bb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MYHOME=/home/dcor/itanlevin\n",
      "env: HF_HUB_CACHE=/home/dcor/itanlevin/.cache/huggingface/hub\n",
      "env: TRANSFORMERS_CACHE=/home/dcor/itanlevin/.cache/huggingface/hub\n"
     ]
    }
   ],
   "source": [
    "%env MYHOME=/home/dcor/itanlevin\n",
    "%env HF_HUB_CACHE=/home/dcor/itanlevin/.cache/huggingface/hub\n",
    "%env TRANSFORMERS_CACHE=/home/dcor/itanlevin/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "854f9609-0e27-42a5-91e4-0c7701b48bed",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e4177ac88141b186e0ef5fc0658eb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9080f2edc5c439887b429f79630af5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import DiffusionPipeline\n",
    "import imageio\n",
    "import cv2\n",
    "import os\n",
    "from ipywidgets import Video\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(\"damo-vilab/text-to-video-ms-1.7b\", torch_dtype=torch.float16, variant=\"fp16\")\n",
    "pipe = pipe.to(\"cuda:1\")\n",
    "\n",
    "# memory optimization\n",
    "pipe.enable_vae_slicing()\n",
    "\n",
    "# \"box1\" \"yoga4\" \"crabdraw\" \"breakdancing\" \"penguin\" \"yoga3\" \"yoyo\" \"car1\" \"fish\" \"box2\" \"sax_play2\" \"dancers3\" \"runner1\" \"biking\" \"hike2\" \"boat2\" \"sax_play1\"\n",
    "\n",
    "# prompt = \"the man sailing the boat, his hands deftly manipulate the oars, while his body shifts subtly to maintain balance, while his boat moves foraward in the river\"\n",
    "prompt = \"A color drawing of a humanoid frog singing and playing the mandolin\"\n",
    "# prompt = \"A fencer stands in the distance in en garde position. His entire body is visible, and is ready to advance\"\n",
    "# note that \"A stickman is jumping.\" or \"A black and white smiley face emoji turns from happy to sad.\" works pretty bad ...\n",
    "video_frames = pipe(prompt, num_frames=24).frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1509a0a-038f-4fc1-8f70-5401267076fb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd4bda584d964ba7a1a90419db4d2924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Video(value=b'\\x00\\x00\\x00 ftypisom\\x00\\x00\\x02\\x00isomiso2avc1mp41\\x00\\x00\\x00\\x08free...')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def frames_to_vid(video_frames, output_vid_path):\n",
    "    len_files = len(video_frames)\n",
    "    writer = imageio.get_writer(output_vid_path, fps=8)\n",
    "    for im in video_frames:\n",
    "        writer.append_data(im)\n",
    "    writer.close()\n",
    "    \n",
    "frames_to_vid(video_frames.squeeze(), output_vid_path=\"stickman_basic_text2vid.mp4\")\n",
    "Video.from_file(\"stickman_basic_text2vid.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c291e1b5-f6ae-4324-ad06-186399a7086d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# SDS Video\n",
    "## the SDS loss is based on Wors-as-Image (a first simple attempt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a0d424-4ed5-431a-a74a-9e1ccc3cc8ab",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import pydiffvg\n",
    "import kornia.augmentation as K\n",
    "from easydict import EasyDict as edict\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import imageio\n",
    "import cv2\n",
    "import os\n",
    "from ipywidgets import Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b8d21f-a617-4038-b1a2-b7e080cbf784",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e54387a-8aa7-4951-abe2-ebbcb7084507",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# ====== video realted utils =======\n",
    "# ==================================\n",
    "def frames_to_vid(video_frames, output_vid_path):\n",
    "    len_files = len(video_frames)\n",
    "    writer = imageio.get_writer(output_vid_path, fps=8)\n",
    "    for im in video_frames:\n",
    "        writer.append_data(im)\n",
    "    writer.close()\n",
    "\n",
    "def render_frames_to_tensor(frames_shapes, frames_shapes_grous, w, h, render, device):\n",
    "    # returns a [16, 256, 256, 3] video tensor\n",
    "    frames_init = []\n",
    "    for i in range(len(frames_shapes)):\n",
    "        shapes = frames_shapes[i]\n",
    "        shape_groups = frames_shapes_grous[i]\n",
    "        scene_args = pydiffvg.RenderFunction.serialize_scene(w, h, shapes, shape_groups)\n",
    "        cur_im = render(w, h, 2, 2, 0, None, *scene_args)\n",
    "    \n",
    "        cur_im = cur_im[:, :, 3:4] * cur_im[:, :, :3] + \\\n",
    "               torch.ones(cur_im.shape[0], cur_im.shape[1], 3, device=device) * (1 - cur_im[:, :, 3:4])\n",
    "        cur_im = cur_im[:, :, :3]\n",
    "        frames_init.append(cur_im)\n",
    "    return torch.stack(frames_init)\n",
    "\n",
    "\n",
    "def save_mp4_from_tensor(frames_tensor, output_vid_path):\n",
    "    # input is a [16, 256, 256, 3] video\n",
    "    frames_copy = frames_tensor.clone()\n",
    "    frames_output = []\n",
    "    for i in range(frames_copy.shape[0]):\n",
    "        cur_im = frames_copy[i]\n",
    "        cur_im = cur_im[:, :, :3].detach().cpu().numpy()\n",
    "        cur_im = (cur_im * 255).astype(np.uint8)\n",
    "        frames_output.append(cur_im)\n",
    "    frames_to_vid(frames_output, output_vid_path=output_vid_path)\n",
    "    \n",
    "    \n",
    "# ==================================\n",
    "# ====== word-as-image utils =======\n",
    "# ==================================\n",
    "def learning_rate_decay(step,\n",
    "                        lr_init,\n",
    "                        lr_final,\n",
    "                        max_steps,\n",
    "                        lr_delay_steps=0,\n",
    "                        lr_delay_mult=1):\n",
    "    if lr_delay_steps > 0:\n",
    "    # A kind of reverse cosine decay.\n",
    "        delay_rate = lr_delay_mult + (1 - lr_delay_mult) * np.sin(\n",
    "            0.5 * np.pi * np.clip(step / lr_delay_steps, 0, 1))\n",
    "    else:\n",
    "        delay_rate = 1.\n",
    "    t = np.clip(step / max_steps, 0, 1)\n",
    "    log_lerp = np.exp(np.log(lr_init) * (1 - t) + np.log(lr_final) * t)\n",
    "    return delay_rate * log_lerp\n",
    "\n",
    "def get_data_augs(cut_size):\n",
    "    augmentations = []\n",
    "    augmentations.append(K.RandomPerspective(distortion_scale=0.5, p=0.7))\n",
    "    augmentations.append(K.RandomCrop(size=(cut_size, cut_size), pad_if_needed=True, padding_mode='reflect', p=1.0))\n",
    "    return nn.Sequential(*augmentations)\n",
    "\n",
    "def init_shapes(svg_path):\n",
    "    parameters = edict()\n",
    "    parameters.point = []\n",
    "    frames_shapes = []\n",
    "    frames_shapes_group = []\n",
    "    \n",
    "    svg = f'{svg_path}.svg'\n",
    "    for i in range(16): # extending the init SVG into 16 frames\n",
    "        canvas_width, canvas_height, shapes_init, shape_groups_init = pydiffvg.svg_to_scene(svg)    \n",
    "        for path in shapes_init:\n",
    "            path.points.requires_grad = True\n",
    "            parameters.point.append(path.points)\n",
    "        frames_shapes.append(shapes_init)\n",
    "        frames_shapes_group.append(shape_groups_init)\n",
    "    return frames_shapes, frames_shapes_group, parameters\n",
    "\n",
    "\n",
    "# ==================================\n",
    "# ======= sds loss (naive) =========\n",
    "# ==================================\n",
    "# TODO: think about a better loss\n",
    "class SDSVideoLoss(nn.Module):\n",
    "    def __init__(self, cfg, device):\n",
    "        super(SDSVideoLoss, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.device = device\n",
    "        self.pipe = DiffusionPipeline.from_pretrained(cfg.model_name, torch_dtype=torch.float16, variant=\"fp16\")\n",
    "        self.pipe = self.pipe.to(self.device)\n",
    "        self.alphas = self.pipe.scheduler.alphas_cumprod.to(self.device)\n",
    "        self.sigmas = (1 - self.pipe.scheduler.alphas_cumprod).to(self.device)\n",
    "\n",
    "        self.text_embeddings = None\n",
    "        self.embed_text()\n",
    "\n",
    "    def embed_text(self):\n",
    "        # tokenizer and embed text\n",
    "        text_input = self.pipe.tokenizer(self.cfg.caption, padding=\"max_length\",\n",
    "                                         max_length=self.pipe.tokenizer.model_max_length,\n",
    "                                         truncation=True, return_tensors=\"pt\")\n",
    "        uncond_input = self.pipe.tokenizer([\"\"], padding=\"max_length\",\n",
    "                                         max_length=text_input.input_ids.shape[-1],\n",
    "                                         return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            text_embeddings = self.pipe.text_encoder(text_input.input_ids.to(self.device))[0]\n",
    "            uncond_embeddings = self.pipe.text_encoder(uncond_input.input_ids.to(self.device))[0]\n",
    "        self.text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "        self.text_embeddings = self.text_embeddings.repeat_interleave(self.cfg.batch_size, 0)\n",
    "        del self.pipe.tokenizer\n",
    "        del self.pipe.text_encoder\n",
    "\n",
    "\n",
    "    def forward(self, x_aug):\n",
    "        # I think that input shape of x should be (1, 16, 3, 256, 256), for 16 frames\n",
    "        sds_loss = 0\n",
    "        x = x_aug * 2. - 1. # encode rendered image\n",
    "        with torch.cuda.amp.autocast():\n",
    "            batch_size, num_frames, channels, height, width = x.shape\n",
    "            x = x.reshape(batch_size * num_frames, channels, height, width) # I think that x shape should be (16, 3, 256, 256), for the VAE encoder\n",
    "            init_latent_z = (self.pipe.vae.encode(x).latent_dist.sample()) # init_latent_z shape is now [16, 4, 32, 32]\n",
    "            frames, channel, h_, w_ = init_latent_z.shape\n",
    "            init_latent_z = init_latent_z[None, :].reshape(batch_size, num_frames, channel, h_, w_).permute(0, 2, 1, 3, 4) # shape should be (1, 4, 16, 32, 32)\n",
    "            \n",
    "        latent_z = self.pipe.vae.config.scaling_factor * init_latent_z  # scaling_factor * init_latents\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            # sample timesteps\n",
    "            timestep = torch.randint(\n",
    "                low=400,\n",
    "                high=min(950, self.cfg.timesteps) - 1,  # avoid highest timestep | diffusion.timesteps=1000\n",
    "                size=(latent_z.shape[0],),\n",
    "                device=self.device, dtype=torch.long)\n",
    "\n",
    "            # add noise\n",
    "            eps = torch.randn_like(latent_z)\n",
    "            # zt = alpha_t * latent_z + sigma_t * eps\n",
    "            noised_latent_zt = self.pipe.scheduler.add_noise(latent_z, eps, timestep)\n",
    "\n",
    "            # denoise\n",
    "            z_in = torch.cat([noised_latent_zt] * 2)  # expand latents for classifier free guidance\n",
    "            timestep_in = torch.cat([timestep] * 2)\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                eps_t_uncond, eps_t = self.pipe.unet(z_in, timestep, encoder_hidden_states=self.text_embeddings).sample.float().chunk(2)\n",
    "            \n",
    "            eps_t = eps_t_uncond + self.cfg.guidance_scale * (eps_t - eps_t_uncond)\n",
    "\n",
    "            # w = alphas[timestep]^0.5 * (1 - alphas[timestep]) = alphas[timestep]^0.5 * sigmas[timestep]\n",
    "            grad_z = self.alphas[timestep]**0.5 * self.sigmas[timestep] * (eps_t - eps)\n",
    "            assert torch.isfinite(grad_z).all()\n",
    "            grad_z = torch.nan_to_num(grad_z.detach().float(), 0.0, 0.0, 0.0)\n",
    "\n",
    "        sds_loss = grad_z.clone() * latent_z\n",
    "        del grad_z\n",
    "\n",
    "        sds_loss = sds_loss.sum(1).mean()\n",
    "        return sds_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27605f10-bd2c-4e55-a543-f6885a7885f9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# Main run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5016482-919d-4064-9944-ec8d34f6a638",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    model_name = \"damo-vilab/text-to-video-ms-1.7b\"\n",
    "    timesteps = 1000\n",
    "    guidance_scale = 100\n",
    "    batch_size = 1\n",
    "    render_size = 256#600\n",
    "    cut_size = 256\n",
    "    num_iter = 501\n",
    "    # num_iter = 10\n",
    "    \n",
    "    lr_base = 0.5 # we need to play with the lr\n",
    "    lr_init = 0.002\n",
    "    lr_final = 0.0008\n",
    "    lr_delay_mult = 0.1\n",
    "    lr_delay_steps = 100\n",
    "    \n",
    "    target = \"svg_input/smile2-01\"\n",
    "    caption = \"A black and white smiley face emoji turns from happy to sad.\"\n",
    "    \n",
    "    \n",
    "# this is the main run\n",
    "cfg = Config()\n",
    "if not os.path.exists(\"./output_videos\"):\n",
    "    os.mkdir(\"./output_videos\")\n",
    "\n",
    "pydiffvg.set_use_gpu(torch.cuda.is_available())\n",
    "device = pydiffvg.get_device()\n",
    "\n",
    "sds_loss = SDSVideoLoss(cfg, device)\n",
    "\n",
    "h, w = cfg.render_size, cfg.render_size\n",
    "data_augs = get_data_augs(cfg.cut_size)\n",
    "\n",
    "render = pydiffvg.RenderFunction.apply\n",
    "# stack the initial svg into 16 frames (for now, the SVG is duplicated and the gradients are backpropogated to all frames)\n",
    "shapes_lst, shape_groups_lst, parameters = init_shapes(svg_path=cfg.target) \n",
    "\n",
    "output_vid_path = \"output_videos/init_vid.mp4\"\n",
    "frames_tensor = render_frames_to_tensor(shapes_lst, shape_groups_lst, w, h, render, device)\n",
    "save_mp4_from_tensor(frames_tensor, output_vid_path)\n",
    "Video.from_file(output_vid_path)\n",
    "\n",
    "num_iter = cfg.num_iter\n",
    "pg = [{'params': parameters[\"point\"], 'lr': cfg.lr_base}]\n",
    "optim = torch.optim.Adam(pg, betas=(0.9, 0.9), eps=1e-6)\n",
    "\n",
    "lr_lambda = lambda step: learning_rate_decay(step, cfg.lr_init, cfg.lr_final, num_iter,\n",
    "                                             lr_delay_steps=cfg.lr_delay_steps,\n",
    "                                             lr_delay_mult=cfg.lr_delay_mult) / cfg.lr_init\n",
    "\n",
    "scheduler = LambdaLR(optim, lr_lambda=lr_lambda, last_epoch=-1)\n",
    "\n",
    "t_range = tqdm(range(num_iter))\n",
    "for step in t_range:\n",
    "    optim.zero_grad()\n",
    "\n",
    "    # render image\n",
    "    vid_tensor = render_frames_to_tensor(shapes_lst, shape_groups_lst, w, h, render, device)\n",
    "    if step % 100 == 0:\n",
    "        save_mp4_from_tensor(vid_tensor, f\"output_videos/{step}.mp4\")\n",
    "    \n",
    "    # cur shape before is (16, 256, 256, 3)\n",
    "    x = vid_tensor.unsqueeze(0).permute(0, 1, 4, 2, 3)  # (16, 256, 256, 3) -> (1, 16, 3, 256, 256)\n",
    "    x = x.repeat(cfg.batch_size, 1, 1, 1, 1)\n",
    "    x_aug = x # for now we skip the augment, but we can add it later (x_aug = data_augs.forward(x))\n",
    "\n",
    "    # compute diffusion loss per pixel\n",
    "    loss = sds_loss(x_aug)\n",
    "\n",
    "    t_range.set_postfix({'loss': loss.item()})\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078c657e-72ec-4447-97dd-2e09edbf63ec",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Video.from_file(\"output_videos/init_vid.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a819a6c-854c-4ca0-8aff-2760121e5185",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Video.from_file(\"output_videos/500.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e916c27-9c8d-4aec-a4a5-8989fcb870f2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "live_sketch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
